;; This buffer is for text that is not saved, and for Lisp evaluation.
;; To create a file, visit it with C-x C-f and enter text in its buffer.

Test implies that we speak at about 200 words a minute


Introduction/Goal/Objectives (2 min ~ 400 words - currently 370 words)
---------------
In this thesis, I conduct an experiment to see whether or not the
No free lunch theorem applies to meta-learning strategies.

When analyzing a dataset, one must decide on an algorithm. This
decision is very often made by an individual with both familiarity in the
datasets relevant problem domain and in machine learning literature.
The deciding of an algorithm by such an expert is, however, not always necessary.
Meta-learning strategies automate the algorithm selection process by use of
stored run statistics for previously analyzed datasets. Given a new dataset a
meta-learning algorithm first uses some measure of similarity between the new
dataset and those datasets that had previously been analyzed, here after
refereed to by the term "metabase". After selecting the dataset of greatest
similarity from the metabase, the meta-learner will return the algorithm that
maximizes the desired run statistic, which is usually the classification accuracy.

The reason a selection must be made on an algorithm at all is that the
performance an algorithm has on a dataset is contingent on what problem that
dataset extends from. This phenomenon was first formally noted by Wolpert and
Macready in "No free lunch theorems for optimization". In the paper they
demonstrate that an algorithm that has enhanced performance in one specific
problem domain will have as a consequence of this enhanced performance degraded
performance in other problem domains. As such, the decision of what algorithm to
use in order to analyze a dataset is non-arbitrary; no one algorithm is best at
analyzing every dataset.

The decision as to what algorithm to use on a dataset itself constitutes a sort
of learning algorithm, with different individuals having different rules behind
their decision of algorithm choice and different meta-learning systems employing
different strategies in order to automate this decision. This implies that the
no free lunch theorem should apply to meta-learning strategies as well, that is
to say no one meta-learning strategy should perform best with every possible
metabase.

As such the goal of this experiment was  to ascertain
whether or not the no free lunch hypothesis applies to meta learning algorithms,
that is to say to ascertain whether or not some meta learning algorithms
strictly dominate other meta learning algorithms.


literature review/theoretical framework (5 min ~ 1000 words - curr 1245 words)
---------------------------
A few terms from literature must be defined before continuing on to a
description of the methodology.

The three metalearning algorithms that I chose for comparison were the Brute
Force Algorithm, the Active Meta learning strategy and the Strategy of Nearest
Learning Curve Comparison. Each of these algorithms takes as its inputs a
metabase and a dataset outside of that metabase and produces a guess at what
algorithm would best classify this new dataset.

The Brute Force approach to meta-learning takes a given metabase as is, then
when given a new dataset applies a given similarity measure (in the case of this
thesis the clustering algorithm) then returns the algorithm that performed
best on the dataset from the metabase that was found to be most similar to the
new dataset.

Introduced in the journal article "Ranking of Classifiers based on Dataset
Characteristics using Active Meta Learning", Active Meta Learning is a
that reduces the cost of generating Meta examples by selecting relevant Meta
examples. It is a meta-learning technique in which a candidate dataset is only
allowed into the metabase if it has a higher uncertainty score relative to its
peers. The relative uncertainty between two datasets is defined by
$$\delta(V_x,d_i,V_x,d_j) = \frac{|V_x,d_i - V_x,d_j|}{Max_{k\neq i}(V_x,d_k)- Min_{k\neq i}(V_x,d_k)}$$
where $V_x,d_k$ is the value of some metaparameter $V_x$ for dataset $d_k$,
$Max_{k\neq i}(V_x,d_k)$ is the maximum value of $V_x,d_k$ when dataset i is
removed and $Min_{k\neq i}(V_x,d_k)$ is its corrosponding minimum. Determining
which dataset has the overall highest uncertainty can be done by summing over
the relative values for each metaparameter, ranking them then collecting their
overall uncertainty scores then choosing the dataset that has the highest score where
$$\delta(V_x,d_i) = \frac{\sum_{j\neq i} |V_x,d_i - V_x,d_j|}{Max_{k\neq i}(V_x,d_k)- Min_{k\neq i}(V_x,d_k)}$$
is the equation representing the overall uncertainty for dataset
$d_i$ in metaparameter $V_x$.

The paper states that this process trains its meta-learner in less time than
a brute force learner and that the resultant meta-learner also has a higher
classification accuracy than a brute force learner. Moreover, due to it having
a smaller metabase, the time it takes for a system that trains its meta-learner
with an active learner will also take less time to classify new datasets than
a brute force learner.

Introduced in the paper "Predicting Relative Performance of Classifiers from
Samples", the strategy of nearest learning curve comparison gathers run
statistics on the datasets within its metabase at various fractions of the full
size of the training portion of that dataset. Categorization of new datasets
is can then be done in two steps. First, the new dataset is trained with some
fractions of its training set with each candidate algorithm. These results are
then compared with the learning curves of each of the sets in the metabase.
The meta-learner then returns the best algorithm of the dataset within the
metabase with which the new datasets learning curves is most similar. The
authors state that this process trains its meta-learner in less time than a
brute force learner and the the resultant meta-learner will also have a higher
classification accuracy than a brute force meta-learner.

The machines learning algorithms that these meta-machine learning strategies
can return as their answers are one of either K-means clustering, neural network,
naive bayes classifier, support vector machine or regression.

Linear regression is a method that asserts that the response is a linear
function of the inputs. This relation takes the following form:

$$ y(\textbf{x}) = \textbf{w}^T\textbf{x} + \epsilon = \sum_{j=1}^{D}w_jx_j + \epsilon $$

where $w^Tx$ represents the inner or scalar product between the input vector $x$
and the model's weight vector $w^T$, and $\epsilon$ is the residual error
between our linear predictions and the true response.

The Naive Bayes classifier algorithm fits a set of data to Bayes theorem with a
strong assumption of feature independance. Given a set of discrete-valued
features $x \in {1,...,K}^D$, we can calculate the class conditional density for
each feature, then with this assumption of independance, generate a guess as to
what the class should be for a new input by multiplying the conditional
likelihood values for each of the new inputs features times the prior on the
desired to be known class, that is to say
$p(y|\textbf{x}) \propto p(y) \sum_{j}^{D}p(x_j|y)$.

The support vector machine is a two-group classification algorithm that attempts
to find a hyper plane that seperates the inputs within the given input space
with a maximum margin of seperation between the hyper plane and the
``support vectors'', those vectors on either side of the hyper plane that are
closest too it.

The objective of the k-means algorithm is to partition a dataset into k groups
such that the points within some group are all closest to
the mean of that group than they are to any other group. A clear
informal explanation of the work that the k-means algorith performs
was given by James McQueen in 1967: "...the k-means procedure
consists of simply starting with k groups each of which consists of a
single random point, and thereafter adding each new point to the
group whose mean the new point is nearest. After a point is added to
a group, the mean of that group is adjusted in order to take account
of the new point. Thus at each stage the k-means are, in fact, the
means of the groups they represent."

A neural network is a type of machine learning algorithm that mimics
the interconnectivity of animal brains in order to automatically
discover rules to classify the inputs given to it. A neural network system
takes the inputs given to it, attempts to make a decision on these inputs,
then assigns, an error value to the decision it made on these inputs based off
how close the answer produced by the system was to its true value. The system
then uses a correction function, often times backpropagation, in order to modify
the weights of the connections between the neurons within the network. This
process will continue until the system has obtained the desire error within its
supply of training data or until it reaches a point where iterations of this
improvement cycle no longer result in improved performance.


methodology (5 min ~ 1000 words)
-----------
The overall goal of this experiment was to determine whether or not one
metalearning strategy within a given set can strictly dominate the others. The
elements required in order to determine this were a set of metalearning
strategies to be compared, sets of metabase datasets on which to apply the
different strategies, and a means to evaluate the results so as to determine the
relative performance of the metalearners. An overview of the procedure this
experiments representive program uses in order to accomplish this now follows.
The program first begins with a set of unprocessed datasets, then extracts the
metafeatures that are required to perform dataset clustering and to run the
active metalearning strategy, storing the results in a local database table.
On completion of this task the program then iterates thru each of the dataset in
our available pool of datasets and extracts run statistics for each of these
datasets; for each available base algorithm a model is trained and
classification is performed on each instance in the dataset that was not used
for the training. The time taken to train the model and the accuracy of the
trained model are then stored within the database. On completion of this task
the program then once again iterates thru the datasets but this time collects
learning curves of the type described earlier, storing them within their own
table in the local database. On completion of this task the program then goes on
to construct 30 collections of the sets of the metabases that will be needed in
order to produce the samples that will be needed for later analysis. On
completion of this task then applies each of the meta-learning strategies to
each of the metabase sets within each of the metabase set collections, storing
the guesses made by a meta-learner within individual database tables. The
program finishes by then running an analysis method on the meta-learners guesses
tables. A more detailed explanation of each of the steps taken by this
experiments program now follows.

All machine learning algorithms boil down to a set of mathematical equations at
the end of the day, with some vector representing a dataset fed in at one end
and a number representing either a catagory or quantity being expelled at the
other. As such any metalearning system must have with it a means to parse the
datasets it means to process. The strategy I employed in order to ensure usable
data was two fold: I first went thru the set of candidate datasets and ensured
non of them was in a format so exotic that they could not be parsed
programatically. Manual examination of the files revealed that those of either
the .data, .svm, or .dat format were agreeable to formating and so it is these
that are processed and used by the parser. These files are then inspected by the
parser, with each datasets column vectors being inspected one by one. Those
column vectors containing only numerical data are left as is, those with any
non-numerical data are assumed to be categorical, with the categories of said
vector being translated to numbers with a unique number being assigned to each
unique string. Rather than storing the numerical representation of each dataset
within the database, the parsed form of a given dataset is crafted when it is
needed, saving disk space and making the project far easier to understand.

- Describe metafeature extraction (currently 198 words) -

The existance of a parser allows us to craft the first table needed for this
experiment which is one containing the meta features of those datasets that are
parsable. Being as how the datasets used within this project have vastly
differing structures along lines such as the number of features and the maximum
and minimum values of these features, the project requires a set of normalized
meta features that are applicable to any possible individual distribution or set
of probability distribution(s). A set of features that meet this criteria are
weighted mean, coefficient of variation, skew, kurtosis, and entropy. The vector
that represents a given dataset is crafted by taking the value of each of these
attributes for each of said datasets features then normalizing them by dividing
by the total number of features within that dataset, that is to say
$$F_{ad} = \frac{\sum_{c=i}^{N}f_{ai}}{N}$$
is the metafeature value $a$ for dataset $d$, $c$ is an iterator across columns
for dataset $d$, $f$ is the value of metafeature $F$ applied to individual
column $i$, and $N$ is the number of columns within dataset $d$. The vector that
represents a given dataset is then determined to be
$V_d = (F_{1d}, F_{2d},...F_{ad})$.

- Describe Metabase Construction (currently 185 words) -

The work of a metalearning algorithm is essentially the applying the things
known from a specific set of datasets towards a new, unlabeled dataset. That
initial set of datasets is known as a meta database, which I shorten to metabase
for convience. The core premise of this experiment is the determination as to
whether or not one metalearning strategy may or may not dominate other
metalearning strategies across a set of of different metabases. As such, this
experiment requires multiple sets of metabases in order to produce samples
that can be used to test our hypothesis. The datasets in a given metabase are
randomly choosen from the set of all datasets then stored in the database. There
are 10 of these per sample, each being a fifth the size of the entire set of
datasets. Testing the performance of a metalearner is done by using the
metalearning strategy with some given metabase and applying it to every other
dataset within the set of datasets. The guesses a given metalearning strategy
makes with some given metabase are then stored within a database table for later
analysis.

- Describe results compilation (currently 90 words) -

Once the guess tables are populated, it is finally possible to compile a table
of results. Each entry in the table notes the metalearning strategy being
evaluated, the metabase collection i.e the sample, the metabase within that
sample that the strategy used in order to analyze its test datasets, and the
accuracy, training time, and rate correct score as its performance metrics,
where the rate correct time measures how often the metalearner makes the correct
guess given the time spent to train it in units of correct guesses per second.


findings(Doc suggests 10, going to do 5 mins ~ 1000 - 2000 words, currently 595 words)
-----------------------
In order to test the null hypothesis, 30 such samples of the kind previously
described were created. The samples and their means can be seen in the following
tables.

If each of the algorithms were truly equal, we would expect the averaged numbers
for each of the positions to be near 3.3. Instead, it appears that the sampler
performed the best, with an average number of first place finishes of 4.5 and the
active strategy performed the worst, with an average number of first place finishes
of 1.7. Whether or not these results fall far enough outside expectation in order to reject
the null hypothesis requires the machinary of classical statistics. Two fairly reliable
measures of how unlikely these results are are the sampling distribution probabilities
and t scores of each of the results.

The following description losely follows the procedure described in \cite{Cohen}.
In it, the author asks you to imagine testing a coin to see whether or not it
is fair by flipping the coin some number of times N. He then asks you to consider whether
some proportion of heads i/N is actually fair. The propability that some proportion of
heads p = i/N is fair can be calculated exactly with the binomial distribution
$$\frac{N!}{i!(N-i)!}r^{i}(1-r)^{N-i}$$.

This situation is analogous to the number of first, second, or third place
finishes some meta-algorithm obtained in this thesis experiment. The probabilty
of proportions for each of the meta-learning algorithms can be seen in table 4.3.

When averaged across samples, we get the following table

The probability of drawing either of the values closest to expectation, 3 or 4,
are 0.26 and 0.22 respectively. The average of all values within this table is
0.15, significantly lower than either expected value. Still, this is not enough
to reject the null hypothesis.

In order to confidantly reject the null hypothesis, we will make use of the
T test.  The t test equation is as follows:
$$t =\frac{\overline{x}-\mu}{\hat{\sigma}_{\overline{x}}} = \frac{\overline{x}-\mu}{\frac{s}{\sqrt{N}}}$$
where s is the sample standard deviation, N is the
number of samples, overscore x is an individual samples mean/calculated value,
and mu is the population mean/expected value.

The idea is simple, take the diffrence between the observed mean of the sample
and the expected mean and normalize this value by the standard deviation of the
samples distribution. This results in some number of sample standard deviations
by which the observed sample distributions mean differs from expectation. The
fewer the number of sample the higher the margin of error in the t scores
estimate, with a margin of error of 0.05 given for estimates made with 30
samples. The critical thresholds for a two tailed t test are -1.96 and 1.96.
If the averaged values of the t scores falls outside these bounds then we can
reject the null hypothesis with a 5 percent margin of error. The standard
deviations and t scores for each of the samples can be seen in the following table.

The following table contains these t scores averaged across samples:

In order to determine whether or not the results as a whole fall significantly
outside of expectation we can take the average of all the averaged t scores
absolute value. This will give as a measure of how much the values observed her
deviate from expectation as a whole. The value we get after under taking this
procedure is 5.11, more than twice the t score value needed in order to
reject the null hypothesis at 5 percent margin of error. We can thus comfortably
reject the null hypothesis.

recommendations (5 min ~ 1000 words, currently 345 words)
-----------------------
In this thesis I proposed that the no free lunch hypotheis might not apply to
meta learning algorithms. In order to test this hypothesis I first built a
system to determine the accuracy of three meta learning strategies:
Exhaustive, Active, and Sampling. To use these strategies, a base of
datasets would first be randomly choosen from our available pool of datasets.
Each strategy would then act on the metabase in its own fashion and make an
estimate as to what algorithm would result in the highest classification accuracy.
Each algorithm would make this guess for each dataset in the pool excluding the
ones in the current base. A new base would then be choosen and the process
repeated 9 more times, giving a number between 0 and 10 for how many times
each algorithm got the most/second most/least number of correct guesses. This process
was repeated 30 times, giving us 30 samples. Statistical analysis was then
performed, giving an average among the absolute values of each of the t scores
of 5.11, allowing us to reject the null hypothesis at a 5 percent margin of error.

Two flaws exist within this experiment that I would correct had I more time: I
was only able to obtain three meta-learning algorithms, and I was only able to
obtain one set of datasets with 88 instances in it. Originally I had desired 5
meta learning algorithms and at least 10 unique sets of datasets. What I am forced to
do here is essentially a form of cross validation, I take a large number of the
possible combination of floor(88 * 0.2) = 17 datasets and segregate them into
groups of 10 and label each of those a sample.  The ideal would have been for
each individual sample to have its own set of 100 or so datasets within it so
as to eliminate any chance of bias within the data. Unfortunately for this to be
a reality I would have had to get my hands on 30 * 100 = 3000 datasets, a task
that I have no idea as to how to begin accomplishing.
