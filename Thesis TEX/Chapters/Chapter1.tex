%Chapter 1
\chapter{Introduction}
\label{Introduction}
Determining what algorithm to use when analyzing a dataset is a problem as
old as machine learning itself. In ``No free lunch theorems for optimization''
Wolpert and Macready demonstrate that the relative performance of any two given
machine learning algorithms will be uniform across all datasets, that is to say
a machine learning algorithms performance is contingent on the problem space in
which the algorithm is operating. As such, the decision of algorithm is not
arbitrary and some strategy must be employed in order to decide
on an algorithm. In some cases, the individuals wishing to perform an analysis
have access to an expert, possibly themselves, that can simply tell them which
algorithm is best in the given situation. In other situations, the individuals
wishing to perform analysis may not have the budget necessary to acquire access
to such an expert, in which case the usage of a meta-learner becomes appropriate.
For instance, at Walmart Labs, meta learning algorithms are used to decide how
best to detect placeholders and to identify fraudulent transactions, all without
the use of manual parameter tunning or even direct algorithm selection \cite{Gupta}.
With a meta-learner, one feeds the meta-learner a dataset, and it returns to the
user what it thinks is the most appropriate machine with which to perform
analysis. To get to the point wherein a decision can be made on new datasets, the
meta-learner itself must first be trained, and this training requires some
sort of learning strategy. This fact suggests that the decision of what
meta-learning strategy to use for some given body of datasets should be
susceptible to the previously mentioned no free lunch theorem, that is to say
that some meta-learning strategies will work better on some given set of
databases than others. The confirmation or denial of this theorem in this
context is the goal of this thesis. Including the current one, this thesis is
comprised of five chapters. In Chapter 2, a review of the base machine and meta
learning strategies used within the experiment is done. Chapter 3 describes the
structure of the experiments code at a high level. Chapter 4 analyzes the
results table in order to determine whether or not one meta-learning strategy
strictly dominates. Chapter 5 presents a summary of the document and addresses a
possible weakness in the results.

\pagebreak
