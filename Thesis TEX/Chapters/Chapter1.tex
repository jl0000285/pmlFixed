%Chapter 1
\chapter{Introduction}
\label{Introduction}
Determining what algorithm to use when analyzing a dataset is a problem as
old as machine learning itself. In ``No free lunch theorems for optimization,''
Wolpert and Macready demonstrate that the relative performance of any two given
machine learning algorithms will be uniform across all datasets, that is to say
a machine learning algorithm's performance is contingent on the problem space in
which the algorithm is operating. As such, the choice of an algorithm is not
arbitrary and some strategy must be employed in order to decide
on an algorithm. In some cases, the individuals wishing to perform an analysis
have access to an expert, possibly themselves, that can simply tell them which
algorithm is best in the given situation. In other situations, the individuals
wishing to perform analysis may not have the budget necessary to acquire access
to such an expert, in which case the usage of a meta learner becomes appropriate.
For instance, at Walmart Labs, meta learning algorithms are used to decide how
best to detect placeholders and to identify fraudulent transactions, all without
the use of manual parameter tuning or even direct algorithm selection \cite{Gupta}.
With a meta learner, one feeds the meta learner a dataset, and it returns to the
user what it thinks is the most appropriate machine with which to perform
analysis. To get to the point wherein a decision can be made on new datasets, the
meta learner itself must first be trained, and this training requires some
sort of learning strategy. This fact suggests that the decision of what
meta learning strategy to use for some given body of datasets should be
susceptible to the previously mentioned no free lunch theorem (NFL), that is to say
that some meta learning strategies will work better on some given set of
databases than others. The confirmation or denial of this theorem in this
context is the goal of this thesis.

There are many factors that may be considered when attempting to optimize
the performance of a machine learning algorithm. One can choose to take special
effort to ensure the parameters of the model are fine-tuned to the type of data
on which one is running the algorithm. Another option to ensure optimum performance
with a machine learning algorithm is to carefully select the features that
one will extract from a given dataset with respect to the desired algorithm.
In the case of this experiment, neither of these topics is addressed; the work
of this thesis is entirely in the comparison of the effectiveness with which
the meta learning strategies utilize a given meta base. Measuring the effects of
base learner parameter-tuning and feature alteration is left to future work as
it is beyond the scope of this experiment.

Including the current one, this thesis is comprised of five chapters. In
Chapter 2, a review of the base machine and meta learning strategies used
within the experiment is done. Chapter 3 describes the structure of the code of
the experiment at a high level. Chapter 4 analyzes the results table in
order to determine whether or not one meta learning strategy strictly dominates.
Chapter 5 presents a summary of the document and proposes possible future work
that can be performed to follow up on the results presented.

\pagebreak
