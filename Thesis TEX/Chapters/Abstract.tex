\documentclass[a4paper,11pt]{article}
\usepackage{amsmath,amsfonts,amsthm,dsfont,amssymb}
\usepackage[blocks]{authblk}
\newenvironment{keywords}{\noindent\textbf{Keywords:}}{}
\newenvironment{classification}{\noindent\textbf{AMS subject classifications.}}{}
\date{}
\newcommand{\email}[1]{\texttt{\small #1}}



\begin{document}
% % % % %--------------------------------------------------------------------
% % % % %          Title of the Paper and Acknowledgement
% % % % %--------------------------------------------------------------------
	\title{A comparison of metalearning strategies
	}
% % % % %--------------------------------------------------------------------
% % % % %         Authors,, Affiliations and email ids
% % % % %--------------------------------------------------------------------

\author{John Liddell}
\affil{ \email{jl0000285@gmail.com}}

% % % % %--------------------------------------------------------------------


\maketitle

\begin{abstract}
Determining what algorithm to use when analyzing a dataset is a problem as old as machine learning itself.
In some cases, the individuals wishing to perform an analysis have access to an expert, possibly themselves,
that can simply tell them which algorithm is best in the given situation. In other situations, the
individuals wishing to perform analysis may not have the budget neccessary to acquire access to such an
expert, in which case the usage of a metalearner becomes appropriate. With a metalearner one feeds
the metalearner a dataset and it returns to the user what it thinks is the most appropriate machine
with which to perform analysis. To get to the point wherein a decision can be made on new datasets the
metalearner itself must first be trained, and this training itself requires some sort of learning strategy.
This fact suggests thath the decision of what metelearning strategy to use for some given body of datasets
should be succeptable to Wolpert's "No free lunch" theorem, that is to say that some metalearning strategies
will work better on some given set of databases than others. The confirmation or denial of this theorem in
this context is the goal of this thesis. The act of utilizing a metalearning strategy generally involves the
following activities: one must take a set of datasets, obtain a measure of how well  those datasets are
analyzed by a given body of algorithms then use the analysis information contained within this "metabase"
to run a decision algorithm that will then generate a resulting machine for consideration. It is the
modification of the measuring and deciding portions of this process that can be considered to represent
differing metalearning strategies.  The system that I will build will first iterate thru the set of
lcomparison strategies, building metabases for each of them and recording how long it took to build each of
these metabases. It will then take a set of test datasets and have machines generated  with a set of
deciding algorithms containing at least K-means clustering with other algorithms to be included if time
allows. The combination of deciding algorithm, dataset, and actual best algorithm will be recorded as a
given metabase makes its decision. Training time vs accuracy analysis will then be performed  to conclude
the validity of this projects core statement.
\end{abstract}

\end{document}
