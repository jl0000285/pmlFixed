;; This buffer is for text that is not saved, and for Lisp evaluation.
;; To create a file, visit it with C-x C-f and enter text in its buffer.

Test implies that we speak at about 200 words a minute


Introduction/Goal/Objectives (2 min ~ 400 words - currently 370 words)
---------------
In this thesis, I conduct an experiment to see whether or not the
No free lunch theorem applies to meta-learning strategies.

When analyzing a dataset, one must decide on an algorithm. This
decision is very often made by an individual with both familiarity in the
datasets relevant problem domain and in machine learning literature. The deciding
of an algorithm by such an expert is, however, not always necessary. Meta-learning
strategies automate the algorithm selection process by use of stored run statistics
for previously analyzed datasets. Given a new dataset a meta-learning algorithm
first uses some measure of similarity between the new dataset and those datasets
that had previously been analyzed, here after refereed to by the term "metabase".
After selecting the dataset of greatest similarity from the metabase, the meta-learner
will return the algorithm that maximizes the desired run statistic, which is usually the
classification accuracy.

The reason a selection must be made on an algorithm at all is that the performance an
algorithm has on a dataset is contingent on what problem that dataset extends from. This
phenomenon was first formally noted by Wolpert and Macready in "No free lunch theorems for 
optimization". In the paper they demonstrate that an algorithm that has enhanced performance
in one specific problem domain will have as a consequence of this enhanced performance degraded
performance in other problem domains. As such, the decision of what algorithm to use in order to
analyze a dataset is non-arbitrary; no one algorithm is best at analyzing every dataset.

The decision as to what algorithm to use on a dataset itself constitutes a sort of learning algorithm,
with different individuals having different rules behind their decision of algorithm choice and
different meta-learning systems employing different strategies in order to automate this decision.
This implies that the no free lunch theorem should apply to meta-learning strategies as well, that is to
say no one meta-learning strategy should perform best with every possible metabase.

As such the goal of this experiment was  to ascertain
whether or not the no free lunch hypothesis applies to meta learning algorithms,
that is to say to ascertain whether or not some meta learning algorithms strictly
dominate other meta learning algorithms.


literature review/theoretical framework (5 min ~ 1000 words - curr 1245 words)
---------------------------
The three metalearning algorithms that I chose for comparison were the Brute Force
Algorithm, the Active Meta learning strategy and the Strategy of Nearest Learning
Curve Comparison. Each of these algorithms takes as its inputs a metabase and a dataset
outside of that metabase and produces a guess at what algorithm would best classify this
new dataset. 

The Brute Force approach to meta-learning takes a given metabase as is, then
when given a new dataset applies a given similarity measure (in the case of this
thesis the clustering algorithm) then returns the algorithm that performed
best on the dataset from the metabase that was found to be most similar to the
new dataset. 

Introduced in the journal article "Ranking of Classifiers based on Dataset 
Characteristics using Active Meta Learning", Active Meta Learning is a
that reduces the cost of generating Meta examples by selecting relevant Meta examples (cite Bhatt here). 
It is a meta-learning technique in which a candidate dataset is only allowed into
the metabase if it has a higher uncertainty score relative to its peers.
The relative uncertainty between two datasets is defined by
$$\delta(V_x,d_i,V_x,d_j) = \frac{|V_x,d_i - V_x,d_j|}{Max_{k\neq i}(V_x,d_k)- Min_{k\neq i}(V_x,d_k)}$$
where $V_x,d_k$ is the value of some metaparameter $V_x$ for dataset $d_k$,
$Max_{k\neq i}(V_x,d_k)$ is the maximum value of $V_x,d_k$ when dataset i is
removed and $Min_{k\neq i}(V_x,d_k)$ is its corrosponding minimum. Determining
which dataset has the overall highest uncertainty can be done by summing over
the relative values for each metaparameter, ranking them then collecting their
overall uncertainty scores then choosing the dataset that has the highest score where
$$\delta(V_x,d_i) = \frac{\sum_{j\neq i} |V_x,d_i - V_x,d_j|}{Max_{k\neq i}(V_x,d_k)- Min_{k\neq i}(V_x,d_k)}$$
is the equation representing the overall uncertainty for dataset
$d_i$ in metaparameter $V_x$.

The paper states that this process trains its meta-learner in less time than
a brute force learner and that the resultant meta-learner also has a higher
classification accuracy than a brute force learner. Moreover, due to it having
a smaller metabase, the time it takes for a system that trains its meta-learner
with an active learner will also take less time to classify new datasets than
a brute force learner. 

Introduced in the paper "Predicting Relative Performance of Classifiers from Samples",
the strategy of nearest learning curve comparison (hereafter shortened to curve comparison)
gathers run statistics on the datasets within its metabase at various fractions
of the full size of the training portion of that dataset. Categorization of new datasets
is can then be done in two steps. First, the new dataset is trained with some fractions of its training set with
each candidate algorithm. These results are then compared with the learning curves of each 
of the sets in the metabase. The meta-learner then returns the best algorithm of the dataset
within the metabase with which the new datasets learning curves is most similar. The authors
state that this process trains its meta-learner in less time than a brute force learner and
the the resultant meta-learner will also have a higher classification accuracy than a brute
force meta-learner. 

The machines learning algorithms that these meta-machine learning strategies
can return as their answers are one of either K-means clustering, neural network,
naive bayes classifier, support vector machine or regression. 

- Linear Regression (currently 209 words) -
Linear regression is one of the most common and oldest machine learning techniques
within literature. It asserts that the response is a linear function of the inputs.
This relation takes the following form:

$$ y(\textbf{x}) = \textbf{w}^T\textbf{x} + \epsilon = \sum_{j=1}^{D}w_jx_j + \epsilon $$

where $w^Tx$ represents the inner or scalar product between the input vector $x$
and the model's weight vector $w^T$, and $\epsilon$ is the residual error between
our linear predictions and the true response.
To fit a linear regression model, the least squares approach is usually used. Given
some ``overdetermined'' linear system (that is to say a system in which there are
more data points than parameters) one can write an expression for the sum of squares of the system $$S(\beta) =(y_1 - \beta x_1)^2 + (y_2 - \beta x_2)^2 + ...
(y_3 - \beta x_3)$$ then take the partial deriviate of this sum of sqaured
deviations with respect to each of the components of $\beta$, set them to zero,
then solve the resulting equations to directly determine what the values of the
parameters are that minimizes the sum of the squared errors of the system. With
linear regression in two dimensions (one dimension in the independant variable and
one dimension in the dependant variable) we see a system with two parameters
$\beta_0 = y intercept$ and $\beta_1 = slope$.

-Naive Bayes (currently 121 words) -
The Naive Bayes classifier algorithm fits a set of data to Bayes theorem with a
strong assumption of feature independance. Given a set of discrete-valued features
$x \in {1,...,K}^D$, we can calculate the class conditional density for each
feature, then with our assumption of independance easily generate a guess at what
the class should be for a new input by multiplying the conditional likelihood values
for each of the new inputs features times the prior on the desired to be known
class, that is to say $p(y|\textbf{x}) \propto p(y) \sum_{j}^{D}p(x_j|y)$. The
calculation of the posterior probability for a new example can be done so
``by hand'' or can be done so from distributions that are infered from the
provided data.

-Support Vector Machines (currently 53 words)-
The support vector machine is a two-group classification algorithm that attempts to
find a hyper plane that seperates the inputs within the given input space with a
maximum margin of seperation between the hyper plane and the ``support vectors'',
those vectors on either side of the hyper plane that are closest too it.

-K Means Clustering (currently 180 words)-
The objective of the k-means algorithm is to partition a dataset into k groups
such that the points within some group are all closest to
the mean of that group than they are to any other group. A clear
informal explanation of the work that the k-means algorith performs
was given by James McQueen in 1967: "...the k-means procedure
consists of simply starting with k groups each of which consists of a
single random point, and thereafter adding each new point to the
group whose mean the new point is nearest. After a point is added to
a group, the mean of that group is adjusted in order to take account
of the new point. Thus at each stage the k-means are, in fact, the
means of the groups they represent." \cite{MacQueen} Formally stated,
given an integer $k$ and a set of $n$ data points in
$\mathbb{R}^{d}$ the K-means algorithm seeks to minimize  $\Phi$, the
over all total summed in class distance between each point and its
closest center such that $\mathbb \Phi = \sum_{x \in X} min_{c \in C}{x-c^{2}}$
\cite{Arthur}.

-Neural Networks (currently 148 words )
A neural network is a type of machine learning algorithm that mimics
the interconnectivity of animal brains in order to automatically
discover rules to classify given inputs. Being that it is one of the most
flexible learning algorithms within literature (actually able to
approximate any continuous function)\cite{Hornik}, its inclusion within a
metalearning system is almost mandatory.  Genrally, such a system
works by first being presented with a set of classified or
unclassified inputs. Said neural network system will then attempt to
make a decision on these inputs on which an error value will then be
assigned. The system will then see some kind of correction function
applied to it. This process will continue until the system has
exhausted its supply of training data, at which point it will
hopefully have discovered a strong set of rules for peforming whatever
work it is that it was designed to perform.


methodology (5 min ~ 1000 words)
-----------
- Describe general idea (currently 217 words) -

The overall goal of this experiment was to determine whether or not one
metalearning strategy within a given set can strictly dominate the others. The
elements required in order to determine this were a set of metalearning
strategies to be compared, sets of metabase datasets on which to apply the
different strategies, and a means to evaluate the results so as to determine the
relative performance of the metalearners. The general flow of the program that
constitutes this experiment begins with a set of unprocessed datasets then
extracts the metafeatures that are required to perform dataset clustering and to
run the active metalearning strategy. The program then constructs 10
metalearning bases with the elements in these sets choosen at random. A run with
every machine algorithm and dataset combination is then performed, with the
results being stored in the runs all table. Learning curves for each
dataset/algorithm combination are then crafted. Finally, enough information now
exists within the the database to start collecting result samples. An individual
result sample is composed of a set of metabases and the results on that set of
metabases. During the experiment, 30 of these samples was gathered. This results
in enough information so that t-test analysis can be performed on the null hypothesis
at a 5 percent margin of error. 

- Describe general parsing plan (currently 230 words) -

All machine learning algorithms boil down to a set of mathematical equations at
the end of the day, with some vector representing a dataset fed in at one end
and a number representing either a catagory or quantity being expelled at the
other. As such any metalearning system must have with it a means to parse the
datasets it means to process. The strategy I employed in order to ensure usable
data was two fold: I first went thru the set of candidate datasets and ensured
non of them was in a format so exotic that they could not be parsed
programatically. Manual examination of the files revealed that those of either
the .data, .svm, or .dat format were agreeable to formating and so it is these
that are processed and used by the parser. These files are then inspected by the
parser, with each datasets column vectors being inspected one by one. Those
column vectors containing only numerical data are left as is, those with any
non-numerical data are assumed to be categorical, with the categories of said
vector being translated to numbers with a unique number being assigned to each
unique string. Rather than storing the numerical representation of each dataset
within the database, the parsed form of a given dataset is crafted when it is
needed, saving disk space and making the project far easier to understand.

- Describe metafeature extraction (currently 198 words) -

The existance of a parser allows us to craft the first table needed for this
experiment which is one containing the meta features of those datasets that are
parsable. Being as how the datasets used within this project have vastly
differing structures along lines such as the number of features and the maximum
and minimum values of these features, the project requires a set of normalized
meta features that are applicable to any possible individual distribution or set
of probability distribution(s). A set of features that meet this criteria are
weighted mean, coefficient of variation, skew, kurtosis, and entropy. The vector
that represents a given dataset is crafted by taking the value of each of these
attributes for each of said datasets features then normalizing them by dividing
by the total number of features within that dataset, that is to say
$$F_{ad} = \frac{\sum_{c=i}^{N}f_{ai}}{N}$$
is the metafeature value $a$ for dataset $d$, $c$ is an iterator across columns
for dataset $d$, $f$ is the value of metafeature $F$ applied to individual
column $i$, and $N$ is the number of columns within dataset $d$. The vector that
represents a given dataset is then determined to be
$V_d = (F_{1d}, F_{2d},...F_{ad})$.

- Describe Metabase Construction (currently 185 words) -

The work of a metalearning algorithm is essentially the applying the things
known from a specific set of datasets towards a new, unlabeled dataset. That
initial set of datasets is known as a meta database, which I shorten to metabase
for convience. The core premise of this experiment is the determination as to
whether or not one metalearning strategy may or may not dominate other
metalearning strategies across a set of of different metabases. As such, this
experiment requires multiple sets of metabases in order to produce samples
that can be used to test our hypothesis. The datasets in a given metabase are
randomly choosen from the set of all datasets then stored in the database. There
are 10 of these per sample, each being a fifth the size of the entire set of
datasets. Testing the performance of a metalearner is done by using the
metalearning strategy with some given metabase and applying it to every other
dataset within the set of datasets. The guesses a given metalearning strategy
makes with some given metabase are then stored within a database table for later
analysis.

- Describe results compilation (currently 90 words) -

Once the guess tables are populated, it is finally possible to compile a table
of results. Each entry in the table notes the metalearning strategy being
evaluated, the metabase collection i.e the sample, the metabase within that
sample that the strategy used in order to analyze its test datasets, and the
accuracy, training time, and rate correct score as its performance metrics,
where the rate correct time measures how often the metalearner makes the correct
guess given the time spent to train it in units of correct guesses per second.


findings(Doc suggests 10, going to do 5 mins ~ 1000 - 2000 words, currently 595 words)
-----------------------
In order to test the null hypothesis, 30 such samples of the kind previously
described were created. The samples and their means can be seen in the following
tables. 

If each of the algorithms were truly equal, we would expect the averaged numbers
for each of the positions to be near 3.3. Instead, it appears that the sampler
performed the best, with an average number of first place finishes of 4.5 and the
active strategy performed the worst, with an average number of first place finishes
of 1.7. Whether or not these results fall far enough outside expectation in order to reject
the null hypothesis requires the machinary of classical statistics. Two fairly reliable
measures of how unlikely these results are are the sampling distribution probabilities
and t scores of each of the results.

The following description losely follows the procedure described in \cite{Cohen}.
In it, the author asks you to imagine testing a coin to see whether or not it
is fair by flipping the coin some number of times N. He then asks you to consider whether
some proportion of heads i/N is actually fair. The propability that some proportion of
heads p = i/N is fair can be calculated exactly with the binomial distribution
$$\frac{N!}{i!(N-i)!}r^{i}(1-r)^{N-i}$$.

This situation is analogous to the number of first, second, or third place
finishes some meta-algorithm obtained in this thesis experiment. The probabilty
of proportions for each of the meta-learning algorithms can be seen in table 4.3.

When averaged across samples, we get the following table

The probability of drawing either of the values closest to expectation, 3 or 4,
are 0.26 and 0.22 respectively. The average of all values within this table is
0.15, significantly lower than either expected value. Still, this is not enough
to reject the null hypothesis.

In order to confidantly reject the null hypothesis, we will make use of the
T test.  The t test equation is as follows:
$$t =\frac{\overline{x}-\mu}{\hat{\sigma}_{\overline{x}}} = \frac{\overline{x}-\mu}{\frac{s}{\sqrt{N}}}$$
where s is the sample standard deviation, N is the
number of samples, overscore x is an individual samples mean/calculated value,
and mu is the population mean/expected value.

The idea is simple, take the diffrence between the observed mean of the sample
and the expected mean and normalize this value by the standard deviation of the
samples distribution. This results in some number of sample standard deviations
by which the observed sample distributions mean differs from expectation. The
fewer the number of sample the higher the margin of error in the t scores
estimate, with a margin of error of 0.05 given for estimates made with 30
samples. The critical thresholds for a two tailed t test are -1.96 and 1.96.
If the averaged values of the t scores falls outside these bounds then we can
reject the null hypothesis with a 5 percent margin of error. The standard
deviations and t scores for each of the samples can be seen in the following table. 

The following table contains these t scores averaged across samples:

In order to determine whether or not the results as a whole fall significantly
outside of expectation we can take the average of all the averaged t scores
absolute value. This will give as a measure of how much the values observed her
deviate from expectation as a whole. The value we get after under taking this
procedure is 5.11, more than twice the t score value needed in order to
reject the null hypothesis at 5 percent margin of error. We can thus comfortably
reject the null hypothesis.

recommendations (5 min ~ 1000 words, currently 345 words)
-----------------------
In this thesis I proposed that the no free lunch hypotheis might not apply to
meta learning algorithms. In order to test this hypothesis I first built a
system to determine the accuracy of three meta learning strategies:
Exhaustive, Active, and Sampling. To use these strategies, a base of
datasets would first be randomly choosen from our available pool of datasets.
Each strategy would then act on the metabase in its own fashion and make an
estimate as to what algorithm would result in the highest classification accuracy.
Each algorithm would make this guess for each dataset in the pool excluding the
ones in the current base. A new base would then be choosen and the process
repeated 9 more times, giving a number between 0 and 10 for how many times
each algorithm got the most/second most/least number of correct guesses. This process
was repeated 30 times, giving us 30 samples. Statistical analysis was then
performed, giving an average among the absolute values of each of the t scores
of 5.11, allowing us to reject the null hypothesis at a 5 percent margin of error.

Two flaws exist within this experiment that I would correct had I more time: I
was only able to obtain three meta-learning algorithms, and I was only able to
obtain one set of datasets with 88 instances in it. Originally I had desired 5
meta learning algorithms and at least 10 unique sets of datasets. What I am forced to
do here is essentially a form of cross validation, I take a large number of the 
possible combination of floor(88 * 0.2) = 17 datasets and segregate them into
groups of 10 and label each of those a sample.  The ideal would have been for
each individual sample to have its own set of 100 or so datasets within it so
as to eliminate any chance of bias within the data. Unfortunately for this to be
a reality I would have had to get my hands on 30 * 100 = 3000 datasets, a task
that I have no idea as to how to begin accomplishing.

